{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A short introduction to Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the bigger and more common issues in proccessing data in neural networks deals with the number of feature spaces one has to deal with and how to best select the features you need to obtain consistant results and model accuracy. For certain problems in machine learning, this could involve feature spaces that strectch into the thousands or the millions. This is where the task of dimensionality reduction could be applied to shorten model training time, or to detect features that might not at frist be caught by more traditional means.\n",
    "\n",
    "Autoencoders work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. Let's expand a bit more on latent-space representations. The goal of encoding your variables into this system is to exploit a certain ‘closeness’ of observed variables based on their context of occurrence to establish meaningful relationship. We expect representations in a space that can capture the 'latent' relationship, and can  either:\n",
    "\n",
    "- Construct encoded (simplififed) representations of the original data\n",
    "\n",
    "- Use the structure of the original data to produce outputs that are similar, yet completely new\n",
    "\n",
    "Examples of where this can be applied are image processing and generation, data visualization of very high feature spaces or complexr  and natural language processing, the latter of which we are currently applying in R & D for our negative comment detection in social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage examples of Autoencoding \n",
    "\n",
    "One of the most famous examples of autoencoding is Word2Vec, developed by a team at google under the direction of Tomáš Mikolov. Word2vec processing works by taking a courpus which is and one-hot-encoding ther\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
